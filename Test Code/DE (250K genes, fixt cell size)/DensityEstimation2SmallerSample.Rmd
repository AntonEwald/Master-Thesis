---
title: "DensityEstimation2SmallerSample"
author: "Anton Holm"
date: '2022-03-17'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(mvtnorm)
library(MASS)
library(dbscan)
library(foreach)
```

This method is the prime conditions for the KDE method. The KDE should become much worse when data is no longer spherical with equal size.

First we generate data from a GMM with 540 gaussians (mimics 540 cells) with a fixt variance of $\approx 3.4$ in both x and y dimensions.
We have on average 250 genes per cell which gives us 135K genes in total. The tissue size is smaller than for the large dataset. We can think of this as taking a smaller piece of a larger tissue and examining it.

```{r, eval = FALSE}
#### Generates a GMM data sample ####
 # No need to run this code
#source("..\\Generate_Data_Script.R")
#gauss2 <- generate_data(n = 135000, clusters = 540, cellwidth = sqrt(11.6), tissue_x = 600, tissue_y = 900)
 #saveRDS(gauss2, file = "GMM_data_small_sample.rdata")
######################################
```

We then extract the quantitites needed.
We also compute the Adjacency Matrix of the data.
```{r}
#### Load the saved data ####
GMM_data2 <- readRDS("GMM_data_small_sample.rdata")
GMM_coordinates2 <- GMM_data2[[1]]
True_density2 <- GMM_data2[[2]]
mu2 <- GMM_data2[[3]]
GMM_sample2 <- GMM_data2[[4]]

#Elements are d_ij
  Adjacency_Matrix2 <- kNNdist(GMM_coordinates2, k = 7, all = TRUE)
  #saveRDS(Adjacency_Matrix, file = "Adjacency_Matrix.rda")
#Adjacency_Matrix <- readRDS("ADjacency_Matrix.rda")
##############################
```

Same density estimators as for large dataset
```{r}
#Density Estimations
Density_Vector1b <- 1/rowSums(Adjacency_Matrix2)
Density_vector2b <- 1/(1 + rowSums(Adjacency_Matrix2^2))

kernel <- function(x, h){
  1/(2*pi*h^2) * exp(-1/2 * (x/h)^2)
}
#Calculate euclidean distance to all k-nearest neighbors (list with each gene-type in the list)
knn_distance2 <- kNNdist(GMM_coordinates2, k = 50, all = TRUE)
kde_dens2 <- kernel(knn_distance2, h = 2.5) %>% 
  rowSums()
##############################
```

Next we want to find the local maximum density peaks around each true mode of the 540 gaussians in the GMM data. We take a small $\epsilon$-neighborhood around the true mode and chose the gene with the largest estimated density as the estimated cell center (or gaussian mode). In the end, we have a dataframe with 540 estimated modes which are different for each of the 3 different density estimatiors.
```{r}

Estimated_modes1b <- matrix(NA, nrow = 540, ncol = 6)
Estimated_modes2b <- matrix(NA, nrow = 540, ncol = 6)
Estimated_modes3b <- matrix(NA, nrow = 540, ncol = 6)

for (i in 1:540) {
  M <- cbind(1:nrow(GMM_coordinates2), apply(GMM_coordinates2, MARGIN = 1, FUN = function(x) sqrt(sum((x[]-mu2[i,])^2))), Density_Vector1b, Density_vector2b, kde_dens2)
  sub_M <- M[M[,2] < 3*sqrt(sqrt(11.6)), ]
  ordered <- sub_M[order(sub_M[,2], decreasing = FALSE), ] %>% 
    cbind(1:nrow(sub_M))
  Estimated_modes1b[i, ] <- ordered[which.max(ordered[,3]), ]
  Estimated_modes2b[i, ] <- ordered[which.max(ordered[,4]), ]
  Estimated_modes3b[i, ] <- ordered[which.max(ordered[,5]), ]

  print(i)
}
```

Make the dataframe of all estimated modes + true modes + statistics
```{r}
Final_mode_df2 <- Estimated_modes1b %>% 
  as_tibble() %>% 
  dplyr::select(V1, V2, V6) %>% 
  rename(c("Datapoint ID" = V1, "Distance to mode" = V2, "KNN index" = V6)) %>% 
  mutate(Method = "Inverse Distance") %>% 
  add_row(`Datapoint ID` = Estimated_modes2b[, 1], `Distance to mode` = Estimated_modes2b[, 2], `KNN index` = Estimated_modes2b[, 6], Method = "Stationary Distribution") %>% 
  add_row(`Datapoint ID` = Estimated_modes3b[, 1], `Distance to mode` = Estimated_modes3b[, 2], `KNN index` = Estimated_modes3b[, 6], Method = "KDE") %>% 
  drop_na()

 saveRDS(Final_mode_df2, file = "Estimated_Modes_vs_True_Circular_Data2.rdata")
```

All above is ground work
----------------------------------------------------------------------------------------
All Below is Analysis


Now we construct two dataframes. One dataframe containing the coordinates of all the estimated modes and the true modes aswell as the position of the estimated mode in relation to its corresponding true mode. 

It was shown that for the KDE estimator, a proportion of its estimated modes lie far away from the true mode. The second dataframe contains these estimated modes, i.e. a subset of the first dataframe.

```{r}
Estimated_vs_True2 <- readRDS("Estimated_Modes_vs_True_Circular_Data2.rdata")

Estimated_Mode_Statistics2 <- Estimated_vs_True2 %>% 
  mutate(x = GMM_coordinates2[`Datapoint ID`, 1],
         y = GMM_coordinates2[`Datapoint ID`, 2],
         mu_x = rep(mu2[, 1], 3),
         mu_y = rep(mu2[, 2], 3)) %>% 
  mutate(diff_x = x-mu_x,
         diff_y = y-mu_y)
# All points around the second peak of KDE
Far_distance2 <- Estimated_vs_True2 %>% 
  mutate(ID = rep(1:540, 3)) %>% 
  filter(`Distance to mode` > 3) 
```


Next we look at the distribution of the distance from the true mode. Here we can see the peak for the KDE but it is not as prominent in this case since the data size is smaller.

```{r}
#Shows distribution of the distance of estimated mode to the true mode
Estimated_vs_True2 %>% 
  ggplot(aes(x = `Distance to mode`, col = Method)) +
  geom_density() +
  labs(title = "Distribution of distance to true mode")
```

Next we plot the spatial information of the estimated modes against their true mode. We can think of it as projecting all true modes on top of origo while keeping the estimated modes position in relation to the true mode. Here the trend from the large dataset is not as prominent. It looks like KDE performs better. This is however the ideal scenario for KDE.

We also calculate the bias and standard deviation of these positions. Here KNN methods has less bias but more variance.

```{r}
#All 3 together
ggplot(Estimated_Mode_Statistics2, aes(x = diff_x, y = diff_y, col = Method)) +
  geom_point()
#1 plot per method
ggplot(Estimated_Mode_Statistics2, aes(x = diff_x, y = diff_y)) +
  geom_point() +
  facet_grid(~Method)

#Table of bias and standard deviation
Estimated_Mode_Statistics2 %>% 
  group_by(Method) %>% 
  summarise(x_bias = mean(diff_x), y_bias = mean(diff_y), x_sd = sd(diff_x), y_sd = sd(diff_y))
```

Now we only look at the true modes of which we had quite poor estimates. We calculate the closest distance of each of these modes to another true mode and plot the distribution of this. The black vertical bar is the radius of the $\epsilon$-neighborhood. We see that we get bad estimates when there is another true mode close to this neighborhood border. This probably mean that our estimated mode is a gene belonging to the other cell. 

```{r}
Far_dist_KDE2 <- Far_distance2 %>% 
  filter(Method == "KDE")
Far_dist_Inv_dist2 <- Far_distance2 %>% 
  filter(Method == "Inverse Distance")
Far_dist_stat_dist2 <- Far_distance2 %>% 
  filter(Method == "Stationary Distribution")
# True modes close to other true mode is harder to find
mu_dist_to_nn2 <- kNNdist(mu2, k = 1)
ggplot(data= NULL) +
  geom_density(aes(x = mu_dist_to_nn2)) +
  geom_density(aes(x = mu_dist_to_nn2[Far_dist_KDE2$ID], col = Method), color = "red") +
  geom_density(aes(x = mu_dist_to_nn2[Far_dist_Inv_dist2$ID], col = Method), color = "blue") +
  geom_density(aes(x = mu_dist_to_nn2[Far_dist_stat_dist2$ID], col = Method), color = "green") +
  geom_vline(xintercept = 3*sqrt(sqrt(11.6)), col = "black", size = 0.8) +
  labs(title = "Distribution of smallest distance between true modes (bad estimates)", x = "Distance of true mode to nearest other true mode")
```





This code is used to plot the different cells and their genes. Mainly for visualization.
```{r}
sample_vec2 <- c()
for(i in 1:length(GMM_sample2)){
  sample_vec2 <- c(sample_vec2, rep(i, GMM_sample2[[i]][1]))
}

GMM_coordinates2 %>% 
  as_tibble() %>% 
  mutate(Gaussian = as.factor(sample_vec2)) %>% 
  filter(Gaussian %in% 1:540) %>% 
  ggplot(aes(x = V1, y = V2, col = Gaussian)) +
  geom_point(size = 1) +
  guides(col = "none")


GMM_coordinates2 %>% 
  as_tibble() %>% 
  mutate(Gaussian = as.factor(sample_vec2)) %>% 
  group_by(Gaussian) %>% 
  summarise(minx = min(V1), maxx = max(V1), miny = min(V2), maxy = max(V2))
```