---
title: "DensityEstimation2"
author: "Anton Holm"
date: '2022-03-09'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(mvtnorm)
library(MASS)
library(dbscan)
library(foreach)
```

This method is the prime conditions for the KDE method. The KDE should become much worse when data is no longer spherical with equal size.

First we generate data from a GMM with 6000 gaussians (mimics 6000 cells) with a fixt variance of 11.6 in both x and y dimensions.
We have on average 250 genes per cell which gives us 1.5M genes in total. 
```{r, eval = FALSE}
#### Generates a GMM data sample ####
 # No need to run this code
#source("..\\Generate_Data_Script.R")
#gauss <- generate_data(n = 1500000, clusters = 6000, cellwidth = 11.6)
# saveRDS(gauss, file = "GMM_data.rdata")
######################################
```

We then extract the quantitites needed.
We also compute the Adjacency Matrix of the data.
```{r}
#### Load the saved data ####
GMM_data <- readRDS("GMM_data.rdata")
GMM_coordinates <- GMM_data[[1]]
True_density <- GMM_data[[2]]
mu <- GMM_data[[3]]

#Elements are d_ij
  #Adjacency_Matrix <- kNNdist(GMM_coordinates, k = 7, all = TRUE)
  #saveRDS(Adjacency_Matrix, file = "Adjacency_Matrix.rda")
Adjacency_Matrix <- readRDS("ADjacency_Matrix.rda")
##############################
```

Next we calculate the density estimations using 3 different methods.
Method 1: $\rho_1 = \frac{1}{\sum_{j = 1}^k d_{ij}}$ where $k$ is the number of neighbors we consider (fixt at 7 for the time being) and $d_{ij}$ is the euclidean distance from gene $i$ to one of its 7 nearest neighbors.
Method 2: $\rho_2 = \frac{1}{\sum_{j = 1}^k 1 + d_{ij}^2}$
Method 3: Kernel Density Estimation with a gaussian kernel of fixt bandwidth of size $2.5$.

```{r}
#### 3 Density Estimations ####

# Density Estimation 1
## Density = 1/sum_j(d_ij)
Density_Vector1 <- 1/rowSums(Adjacency_Matrix)

# Density Estimation 2
## Density = 1/(1+sum_j(d_ij)^2)
Density_vector2 <- 1/(1 + rowSums(Adjacency_Matrix^2))

# Density Estimation 3
## Gaussian Kernel Estimation
kernel <- function(x, h){
  1/(2*pi*h^2) * exp(-1/2 * (x/h)^2)
}

#Calculate euclidean distance to all 50-nearest neighbors
knn_distance <- kNNdist(GMM_coordinates, k = 50, all = TRUE)
kde_dens <- kernel(knn_distance, h = 2.5) %>% 
  rowSums()
##############################
```

Next we want to find the local maximum density peaks around each true mode of the 6000 gaussians in the GMM data. We take a small $\epsilon$-neighborhood around the true mode and chose the gene with the largest estimated density as the estimated cell center (or gaussian mode). In the end, we have a dataframe with 6000 estimated modes which are different for each of the 3 different density estimatiors.
```{r}

Estimated_modes1 <- matrix(NA, nrow = 6000, ncol = 6)
Estimated_modes2 <- matrix(NA, nrow = 6000, ncol = 6)
Estimated_modes3 <- matrix(NA, nrow = 6000, ncol = 6)

for (i in 1:6000) {
  M <- cbind(1:nrow(GMM_coordinates), apply(GMM_coordinates, MARGIN = 1, FUN = function(x) sqrt(sum((x[]-mu[i,])^2))), Density_Vector1, Density_vector2, kde_dens)
  sub_M <- M[M[,2] < 3*sqrt(11.6), ]
  ordered <- sub_M[order(sub_M[,2], decreasing = FALSE), ] %>% 
    cbind(1:nrow(sub_M))
  Estimated_modes1[i, ] <- ordered[which.max(ordered[,3]), ]
  Estimated_modes2[i, ] <- ordered[which.max(ordered[,4]), ]
  Estimated_modes3[i, ] <- ordered[which.max(ordered[,5]), ]

  print(i)
}
```

```{r}
Final_mode_df <- Estimated_modes1 %>% 
  as_tibble() %>% 
  dplyr::select(V1, V2, V6) %>% 
  rename(c("Datapoint ID" = V1, "Distance to mode" = V2, "KNN index" = V6)) %>% 
  mutate(Method = "Inverse Distance") %>% 
  add_row(`Datapoint ID` = Estimated_modes2[, 1], `Distance to mode` = Estimated_modes2[, 2], `KNN index` = Estimated_modes2[, 6], Method = "Stationary Distribution") %>% 
  add_row(`Datapoint ID` = Estimated_modes3[, 1], `Distance to mode` = Estimated_modes3[, 2], `KNN index` = Estimated_modes3[, 6], Method = "KDE") %>% 
  drop_na()

## saveRDS(Final_mode_df, file = "Estimated_Modes_vs_True_Circular_Data.rdata")
```



All above is ground work
----------------------------------------------------------------------------------------
All Below is Analysis



Now we construct two dataframes. One dataframe containing the coordinates of all the estimated modes and the true modes aswell as the position of the estimated mode in relation to its corresponding true mode. 

It was shown that for the KDE estimator, a proportion of its estimated modes lie far away from the true mode. The second dataframe contains these estimated modes, i.e. a subset of the first dataframe.

```{r}
Estimated_vs_True <- readRDS("Estimated_Modes_vs_True_Circular_Data.rdata")

Estimated_Mode_Statistics <- Estimated_vs_True %>% 
  mutate(x = GMM_coordinates[`Datapoint ID`, 1],
         y = GMM_coordinates[`Datapoint ID`, 2],
         mu_x = rep(mu[, 1], 3),
         mu_y = rep(mu[, 2], 3)) %>% 
  mutate(diff_x = x-mu_x,
         diff_y = y-mu_y)

# All points around the second peak of KDE
Far_distance <- Estimated_vs_True %>% 
  mutate(ID = rep(1:6000, 3)) %>% 
  filter(`Distance to mode` > 9.5) 
```


Next we look at the distribution of the distance from the true mode. Here we can see the peak for the KDE. We also calculate the mean and standard deviation of the distance from estimated mode to the true mode. KDE have lower mean (2.35) vs 2.74 of the stationary distribution but have a standard deviation of 2.47 vs 2.18 of the stationary distribution.
```{r}
#Shows distribution of the distance of estimated mode to the true mode
Estimated_vs_True %>% 
  ggplot(aes(x = `Distance to mode`, col = Method)) +
  geom_density() +
  labs(title = "Distribution of distance to true mode")

mean_sd_above_distribution <- Estimated_vs_True %>% 
  group_by(Method) %>% 
  summarise(mean = mean(`Distance to mode`),
            sd = sd(`Distance to mode`))

```


Next we plot the spatial information of the estimated modes against their true mode. We can think of it as projecting all true modes on top of origo while keeping the estimated modes position in relation to the true mode. Here we can once again see that the KDE estimator has less spread in the main cluster near origo but at the same time have more estimated modes far away (at the border of the epsilon neighborhood). 

We also calculate the bias and standard deviation of these positions. Graph density methods have more bias in x-axis while KDE have more in y-axis. All in all, all 3 methods are more or less unbiased with similar standard deviation. 

```{r}
#All 3 together
ggplot(Estimated_Mode_Statistics, aes(x = diff_x, y = diff_y, col = Method)) +
  geom_point()
ggsave(file = "Spatial_Dist_Estimated_Modes.png")
#1 plot per method
ggplot(Estimated_Mode_Statistics, aes(x = diff_x, y = diff_y)) +
  geom_point() +
  facet_grid(~Method)

#Table of bias and standard deviation
Estimated_Mode_Statistics %>% 
  group_by(Method) %>% 
  summarise(x_bias = mean(diff_x), y_bias = mean(diff_y), x_sd = sd(diff_x), y_sd = sd(diff_y))
```


Now we only look at the true modes of which we had quite poor estimates. We calculate the closest distance of each of these modes to another true mode and plot the distribution of this. The black vertical bar is the radius of the $\epsilon$-neighborhood. We see that we get bad estimates when there is another true mode close to this neighborhood border. This probably mean that our estimated mode is a gene belonging to the other cell. 

```{r}
Far_dist_KDE <- Far_distance %>% 
  filter(Method == "KDE")
Far_dist_Inv_dist <- Far_distance %>% 
  filter(Method == "Inverse Distance")
Far_dist_stat_dist <- Far_distance %>% 
  filter(Method == "Stationary Distribution")
# True modes close to other true mode is harder to find
mu_dist_to_nn <- kNNdist(mu, k = 1)
ggplot(data = NULL) +
  geom_density(aes(x = mu_dist_to_nn)) +
  geom_density(aes(x = mu_dist_to_nn[Far_dist_KDE$ID], col = Method), color = "red") +
  geom_density(aes(x = mu_dist_to_nn[Far_dist_Inv_dist$ID], col = Method), color = "blue") +
  geom_density(aes(x = mu_dist_to_nn[Far_dist_stat_dist$ID], col = Method), color = "green") +
  geom_vline(xintercept = 3*sqrt(11.6), col = "black", size = 0.8) +
  labs(title = "Distribution of smallest distance between true modes", x = "Distance of true mode to nearest other true mode")

#####################################################################
```



Above is analysis
--------------------------------------------------------------------------------
Below is notes for myself


Create a nicer pipeline for estimating densities.

What does the sigma mean, i.e. variance of estimated mode? How do we calculate them? What is the interpretation of them?

How do we find local maximums?

Try data with alternating covariance matrix and compare results.

On Christophers data: Try sctransform + clustering and no normalization + clustering and compare.


Check the distance between every true mode

Check bias and variance of the gaussians points
THink of it as taking each true mode at origin and see how the points gets distributed

For meeting:
  - How do we decide on k?
  - Show distribution of which k-NN was estimated as the mode
  - How do we apply the rank spatial preservence statistics in our case?

What I did last time:
  - I checked the distribution of which neighbor to the true modes is the estimated mode for 1/sum(d_ij)

Next:
  - Do the same for stationary distribution aswell