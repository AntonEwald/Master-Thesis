---
title: "New Tests"
author: "Anton Holm"
date: '2022-03-25'
output: html_document
---



```{r}
source("full_method.R")
```

Now fix the grid loop for calculating delta faster
We will not calculate the edges. This is fine since these are wrong anyways since the images potentially does not include many of its neighbors.


Grid loop for analysis.
Functional one in scripts (for manual cluster detection)
Below is testing for automatic decisions
```{r}
data_c250 <- readRDS("DE (250K genes, varying cellsize + cellshape)//(DATA) n = 57.000, c = 250, tissue = 400, VS.rdata")
############# Find average length between clusters #####################
daata <- data_c250
mode_distances <- dista(daata$Mu, daata$Mu)
diag(mode_distances) <- NA
avg_mode_dist <- mean(apply(mode_distances, 1, min, na.rm = TRUE))
L <- 3*avg_mode_dist
#######################################################################

rho = calc_rho(daata$Coordinates, k=7, estimator = 2)
#Order by rho
sorted_rho <- cbind(daata$Coordinates, rho, 1:length(rho)) %>% 
  .[order(.[,3], decreasing = TRUE), ]
sorted <- cbind(1:length(rho), sorted_rho)

xmin <- min(daata$Coordinates[,1])
xmax <- max(daata$Coordinates[,1])
ymin <- min(daata$Coordinates[,2])
ymax <- max(daata$Coordinates[,2])
xlength <- xmax - xmin
ylength <- ymax - ymin
grid_size <- L/3
xgrids <- ceiling(xlength/grid_size)
ygrids <- ceiling(ylength/grid_size)
deltas <- matrix(NA, ncol = 4, nrow = nrow(daata$Coordinates))
cluster_centers <- c()
for (i in 2:(ygrids-1)){
  for (j in 2:(xgrids-1)){
    
    large_grid <- sorted[sorted[, 2] < xmin + (j+1)*grid_size &
                       sorted[, 2] >= xmin + (j-2)*grid_size & 
                     sorted[, 3] < ymin + (i+1)*grid_size &
                       sorted[, 3] >= ymin + (i-2)*grid_size, ]
      
    small_grid <- sorted[sorted[, 2] < xmin + j*grid_size &
                       sorted[, 2] >= xmin + (j-1)*grid_size & 
                     sorted[, 3] < ymin + i*grid_size &
                       sorted[, 3] >= ymin + (i-1)*grid_size, ]
    
    if (is.vector(small_grid) == TRUE){ #Only one point within our smaller grid
      # It should be an outlier most likely so we don't need to consider it
    }
    else if (nrow(small_grid) < 6){
     # Dont add anything
    }
    else {
    #Atleast 6 points in the small grid
      sub_stats <- matrix(NA, ncol = 3, nrow = nrow(small_grid))
      #Calculate our deltas for the small grid
      for (k in 1:nrow(small_grid)){ #For every point in the small grid
        if (sum(large_grid[,1] < small_grid[k, 1]) == 0){ #There is no point with larger density
          deltas[small_grid[k, 5], 1] = L #Give max delta
          deltas[small_grid[k, 5], 2] = i
          deltas[small_grid[k, 5], 3] = j
          deltas[small_grid[k, 5], 4] = small_grid[k, 1]

          sub_stats[k, 1] <- small_grid[k, 5] 
          sub_stats[k, 2] <- L 
          sub_stats[k, 3] <- small_grid[k, 4]
        }
        
        else if (sum(large_grid[,1] < small_grid[k,1]) == 1){
          delta = sqrt((large_grid[large_grid[,1] < small_grid[k,1], ][2] - small_grid[k,2])^2 + (large_grid[large_grid[,1] < small_grid[k,1], ][3] - small_grid[k,3])^2)
          deltas[small_grid[k,5], 1] <- delta
          deltas[small_grid[k, 5], 2] = i
          deltas[small_grid[k, 5], 3] = j
          deltas[small_grid[k, 5], 4] = small_grid[k, 1]
          sub_stats[k, 1] <-small_grid[k, 5]
          sub_stats[k, 2] <- delta
          sub_stats[k, 3] <- small_grid[k, 4]

        }
        else{
          delta = min(apply(large_grid[large_grid[,1] < small_grid[k,1], ][,2:3], 1, function(x) sqrt(sum((x-small_grid[k,2:3])^2))))
          deltas[small_grid[k, 5], 1] <- delta
          deltas[small_grid[k, 5], 2] = i
          deltas[small_grid[k, 5], 3] = j
          deltas[small_grid[k, 5], 4] = small_grid[k, 1]
          sub_stats[k, 1] <- small_grid[k, 5]
          sub_stats[k, 2] <- delta
          sub_stats[k, 3] <- small_grid[k, 4]
        }
      }
      #Here we should check for density peaks
      cluster_centers <- c(cluster_centers, auto_peak_finder(sub_stats))
    }
  }
  print(i)
}

ggplot(data = NULL) +
  geom_point(aes(x = daata$Coordinates[,1], y = daata$Coordinates[,2], col = daata$Samples)) +
  geom_point(aes(x = daata$Coordinates[cluster_centers,1], y = daata$Coordinates[cluster_centers,2]), color = "red", size = 2) +
  geom_point(aes(x = daata$Coordinates[center_id,1], y = daata$Coordinates[center_id,2]), color = "blue", size = 1) +
  geom_vline(xintercept = xmin + 0:xgrids*grid_size) +
  geom_hline(yintercept = ymin+ 0:ygrids*grid_size) +
  geom_point(aes(x = daata$Coordinates[small_grid[,5], 1], y = daata$Coordinates[small_grid[,5], 2]), color = "purple") +
  guides(col = "none")


```


Function without derivatives that find the cluster centers based on "counts" of occurance of nr of clusters, i.e. length of a bar. Should fix so that we use derivatives with some smoothing. Smoothing is needed since what is we have 100 ones, 5 twos, 5 threes, 99 fours, 99 fives, 5 sixes, 5 sevens and so on. Then we should pick 4 or 5 clusters, not just 1. 
```{r}
auto_peak_finder <- function(sub_stats){
delta = sub_stats[,2]
  rho = sub_stats[,3]
  iso <- as.stepfun(isoreg(x = log(rho), y = -log(delta))) #monotonic regression line (minus on y cause we have decreasing correlation)
  a = -iso(log(rho)) # Minus again to revert back the minus above
  #Find which constant to use for the decision line ln(rho) + a*ln(delta) = const
  threshold <-  seq(0.1, log(L), by = 0.1) 
  clusters <- c()
  
  for (i in 1:length(threshold)){
    clusters[i] <- sum(log(delta) > (threshold[i] + exp(a)))
  }
nr_clusters <- as.numeric(sort(table(clusters), decreasing = TRUE)[1] %>% as.data.frame() %>% rownames()) #Cluster with most occurence
# Add derivative automation instead
  
stuffs <- cbind(threshold, clusters) 
chosen_thresh <- median(stuffs[stuffs[,2] == nr_clusters, 1]) #Take the threshold in the middle of the peak and valley
peak_id <- sub_stats[which(delta > (chosen_thresh*rho)^a), 1] #Datapoint ID for cluster centers
return(peak_id)
}
```

Below is for testing how monotonic regression works in decision graph
Also used to try and automate threshold decision.
Here I need to decide on how to fix min and max of threshold.
```{r}
rho = sub_stats[,3]
delta = sub_stats[,2]

iso <- as.stepfun(isoreg(x = log(rho), y = -log(delta)))
a = -iso(log(rho))
# Some analysis on what threshold to pick
y <- exp(a) +1
y5 <- exp(a) + 3
y4 <- exp(a) +6
y2 <- exp(a) +9
y3 <- exp(a) +14
y6 <- exp(a) +18


ggplot(data = NULL) +
  geom_point(aes(log(rho), (delta))) +
  geom_line(aes(log(rho), y), color = "yellow") +
      geom_line(aes(log(rho), y5), color = "orange")+
      geom_line(aes(log(rho), y4), color = "red")   +
geom_line(aes(log(rho), y2), color = "light blue") +
      geom_line(aes(log(rho), y3), color = "blue")+
      geom_line(aes(log(rho), y6), color = "purple")
```
























Below is the findcenter function but there is a better one in scripts
```{r}
findCenters <- function(sub_stats){
delta_sub = sub_stats[,2]
rho_sub = sub_stats[,3]
a <- lm(log(delta_sub)~log(rho_sub))$coefficient[[2]]
  #Find which constant to use for the decision line ln(rho) + a*ln(delta) = const
  threshold <-  seq(0, 1, by = 0.001)
  clusters <- c()
  
  for (i in 1:length(threshold)){
    clusters[i] <- sum(delta_sub > (threshold[i]*rho_sub)^a)
  }
library(pracma)
  #Project decisiongraph on regression line to find peaks and valleys
  b <- lm(clusters ~ threshold)$coefficient[[2]]
  new_coefs <- cbind(as.numeric(threshold), as.numeric(clusters - threshold*b))
  #Find all peaks and valleys
  p <- findpeaks(new_coefs[,2])
  v <- findpeaks(-new_coefs[,2])
  #Find the best candidate for threshold
  #If we start with a valley remove it for easier computation (only interested in peak vs next valley)
  if (p[1,2] > v[1,2]| length(p) == 0) {
    p = rbind(c(0, 1, 0, 0), p)
  }
  #Add a minimum at the end if the curve was still going down
  
  if (nrow(p) > nrow(v)){
    v = rbind(v, c(0, length(threshold), 0, 0))
  }
  ind_pv <- which.max(v[, 2] - p[, 2])
  ind_bin <- (p[ind_pv, 2] + v[ind_pv,2])/2
  Threshold <- stats$coefs[[ind_bin, 1]]
  #Threshold found
  center_id = sub_stats[which(delta > (Threshold*rho)^a),1]
  return(center_id = center_id)
}

```


Nothin Important below
```{r}
source("full_method.R")

my_find <- findThreshold(stats)

plot(statsc250)
abline(v = my_find$Threshold)

fit <- ksmooth(my_find$new_coefs[,1], my_find$new_coefs[,2], kernel = "normal", bandwidth = 0.1)
p <- findpeaks(fit$y)
v <- findpeaks(-fit$y)
ggplot(data = NULL, aes(x = my_find$new_coefs[,1], y = fit$y)) +
  geom_line()+
  geom_point(aes(x = my_find$new_coefs[p[,2],1], y = my_find$new_coefs[p[,2],2]), shape = "+", color = "blue", size = 3) +
  geom_point(aes(x = my_find$new_coefs[v[,2],1], y = my_find$new_coefs[v[,2],2]), shape = "-", color = "red", size = 3)




test_clustering <- clustering(stats, my_find$Threshold)
test_clustering$centerID


ggplot(data = NULL, aes(x = statsc250$rho, y = deltas)) +
  geom_point()

as_tibble(cbind(statsc250$rho, data_c250$Samples)) %>% 
  group_by(V2) %>% 
  summarise(max = max(V1)) %>% 
  arrange(max)
 
ggplot(data = NULL, aes(x = data_c250$Coordinates[,1], y = data_c250$Coordinates[,2], col = deltas)) + geom_point()


ggplot(data = NULL) +
  geom_point(aes(x = daata$Coordinates[,1], y = daata$Coordinates[,2], col = daata$Samples)) +
  guides(col = "none") +
  geom_vline(xintercept = xmin + 0:(xgrids)*grid_size) +
  geom_hline(yintercept = ymin + 0:(ygrids)*grid_size)

plot(statsc250)
abline(v = 0.095)
``` 


Last Last time:
 - Fixed automatic way to find threshold
  - used kernel regression here
 - Fixed loop to loop over grids
 
Last Time:
 - Made the threshold finding for each grid instead of one at the end
 - The loop now calculates correct deltas, exactly the same as for the method of doing entire images at once.
 - I get all the same cluster centers if I dont do automatic and do all at the end. Only miss those at the boundries as we should.
 
Next time: 
- Calculating Deltas by grids works perfect
- Finding number of clusters manually works perfect
- Finding number of clusters automatically is problematic. Monotonic regression works good now.
- Needs to be done: Find how to decide on threshold intervals and step size!!!


 - Monotonic Regression on finding the slope a
 - Clustering as used by fast search paper (it is hard clustering)
 - 